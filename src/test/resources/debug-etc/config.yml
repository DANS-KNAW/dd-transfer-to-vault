#
# dd-transfer-to-vault
#
collect:
  inboxes:
    - name: Test inbox 1
      path: data/tmp/transfer-inboxes/inbox1
    - name: Test_inbox 2
      path: data/tmp/transfer-inboxes/inbox2
  # polling interval in milliseconds
  pollingInterval: 500
  taskQueue:
    nameFormat: "collect-worker-thread-%d"
    maxQueueSize: 5000
    # Number of threads will be increased when maxQueueSize is exceeded.
    minThreads: 1
    # No more than maxThreads will be created though
    maxThreads: 10
    # Threads will die after 60 seconds of idleness
    keepAliveTime: 60 seconds

nbnRegistration:
  catalogBaseUrl: https://catalog.vault.datastations.nl/dataset
  registrationInterval: 1000
  gmh:
    url: "https://resolver.tgharvester.dans.knaw.nl/gmh-registration-service/nbn"
    token: # Fill in a valid token, generated via the token endpoint of the GMH server
    httpClient:
      timeout: 30s
      connectionTimeout: 15s
      timeToLive: 1h
      retries: 2
      # The GMH server does not handle GZIP compression
      gzipEnabled: false

extractMetadata:
  inbox: data/tmp/metadata-inbox
  # polling interval in milliseconds
  pollingInterval: 500
  taskQueue:
    nameFormat: "metadata-worker-thread-%d"
    maxQueueSize: 5000
    # Number of threads will be increased when maxQueueSize is exceeded.
    minThreads: 1
    # No more than maxThreads will be created though
    maxThreads: 10
    # Threads will die after 60 seconds of idleness
    keepAliveTime: 60 seconds

sendToVault:
  inbox: data/tmp/send-to-vault-inbox
  pollingInterval: 500
  work: data/tmp/send-to-vault-work
  maxBatchSize: 10000
  outbox: data/tmp/data-vault-inbox

confirmArchived:
  cron: '0 0/15 * * * ?' # cron expression for triggering a polling round
  vaultServiceEndpoint: http://localhost:20305/api
  taskQueue:
    nameFormat: "confirm-thread-%d"
    maxQueueSize: 5000
    # Number of threads will be increased when maxQueueSize is exceeded.
    minThreads: 1
    # No more than maxThreads will be created though
    maxThreads: 10
    # Threads will die after 60 seconds of idleness
    keepAliveTime: 60 seconds

vaultCatalog:
  url: http://localhost:20305
  httpClient:
    timeout: 10s
    connectionTimeout: 1min
    # disable chunked encoding because it breaks the multipart/form-data headers:
    chunkedEncodingEnabled: false
    timeToLive: 1h
    cookiesEnabled: false
    maxConnections: 128
    maxConnectionsPerRoute: 128
    keepAlive: 0ms
    retries: 0
    userAgent: dd-transfer-to-vault

dataVault:
  url: http://localhost:20365
  httpClient:
    timeout: 10s
    connectionTimeout: 1min
    # disable chunked encoding because it breaks the multipart/form-data headers:
    chunkedEncodingEnabled: false
    timeToLive: 1h
    cookiesEnabled: false
    maxConnections: 128
    maxConnectionsPerRoute: 128
    keepAlive: 0ms
    retries: 0
    userAgent: dd-transfer-to-vault


database:
  driverClass: org.hsqldb.jdbcDriver
  url: jdbc:hsqldb:hsql://localhost:9002/dd-transfer-to-vault
  logValidationErrors: true
  # See: https://stackoverflow.com/questions/10684244/dbcp-validationquery-for-different-databases
  validationQuery: SELECT * FROM INFORMATION_SCHEMA.SYSTEM_TABLES
  properties:
    hibernate.dialect: 'org.hibernate.dialect.HSQLDialect'
    hibernate.hbm2ddl.auto: update

logging:
  level: INFO
  loggers:
    'nl.knaw.dans': DEBUG
    'org.hibernate.engine.internal.StatisticalLoggingSessionEventListener': 'OFF'
  appenders:
    - type: file
      archive: false
      currentLogFilename: 'data/dd-transfer-to-vault.log'
    - type: console
      logFormat: "%-5p [%d{ISO8601}] [%t] %c: %m%n%rEx"
