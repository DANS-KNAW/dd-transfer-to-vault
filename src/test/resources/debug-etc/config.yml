#
# dd-transfer-to-vault configuration file
#
server:
  applicationContextPath: /
  adminContextPath: /
  applicationConnectors:
    - type: http
      port: 20350
  adminConnectors:
    - type: http
      port: 20351
  # Logs the requests to the Jetty http server
  requestLog:
    appenders:
      - type: file
        archive: false
        timeZone: system
        currentLogFilename: data/request.log

transfer:
  datastation: Test Datastation
  # Inbox for incoming DVEs
  collectDve:
    inbox:
      path: data/01_transfer-inbox/inbox
      pollingInterval: 500ms
    outbox:
      processed: data/02_extract-metadata/inbox
      failed: data/01_transfer-inbox/failed
  extractMetadata:
    inbox:
      path: data/02_extract-metadata/inbox
      pollingInterval: 500ms
    outbox:
      processed: data/03_send-to-vault/inbox
      failed: data/02_extract-metadata/outbox/failed
      rejected: data/02_extract-metadata/outbox/rejected
    taskQueue:
      nameFormat: "extract-metadata-worker-%d"
      maxQueueSize: 5000
      # Number of threads will be increased when maxQueueSize is exceeded.
      minThreads: 1
      # No more than maxThreads will be created though
      maxThreads: 3
      # Threads will die after 60 seconds of idleness
      keepAliveTime: 60 seconds
  sendToVault:
    inbox:
      path: data/03_send-to-vault/inbox
      pollingInterval: 500ms
    outbox:
      processed: data/03_send-to-vault/outbox/processed
      failed: data/03_send-to-vault/outbox/failed
    dataVault:
      # The current batch being assembled
      currentBatchWorkingDir: data/03_send-to-vault/work
      # Create import batches under this directory
      batchRoot: data/04_data-vault/inbox
      # If the batch size exceeds this threshold, issue the import request and start a new batch
      threshold: 100MB

nbnRegistration:
  catalogBaseUrl: http://dev.catalog.vault.datastations.nl/dataset
  registrationInterval: 1000
  gmh:
    url:
    token: # Fill in a valid token, generated via the token endpoint of the GMH server
    httpClient:
      timeout: 30s
      connectionTimeout: 15s
      timeToLive: 1h
      retries: 2
      # The GMH server does not handle GZIP compression
      gzipEnabled: false
      userAgent: dd-transfer-to-vault

vaultCatalog:
  url: http://dev.transfer.dans-data.nl:20305
  httpClient:
    timeout: 10s
    connectionTimeout: 1min
    # disable chunked encoding because it breaks the multipart/form-data headers:
    chunkedEncodingEnabled: false
    timeToLive: 1h
    cookiesEnabled: false
    maxConnections: 128
    maxConnectionsPerRoute: 128
    keepAlive: 0ms
    retries: 0
    userAgent: dd-transfer-to-vault

dataVault:
  url: http://localhost:20365
  httpClient:
    timeout: 10s
    connectionTimeout: 1min
    # disable chunked encoding because it breaks the multipart/form-data headers:
    chunkedEncodingEnabled: false
    timeToLive: 1h
    cookiesEnabled: false
    maxConnections: 128
    maxConnectionsPerRoute: 128
    keepAlive: 0ms
    retries: 0
    userAgent: dd-transfer-to-vault

database:
  driverClass: org.postgresql.Driver
  url: jdbc:postgresql://dev.transfer.dans-data.nl:5432/dd_transfer_to_vault
  user: dd_transfer_to_vault
  password: dd_transfer_to_vault
  logValidationErrors: true
  properties:
    hibernate.dialect: 'org.hibernate.dialect.PostgreSQL95Dialect'
    hibernate.hbm2ddl.auto: update

#
# See https://www.dropwizard.io/en/latest/manual/configuration.html#logging
#
logging:
  level: INFO
  appenders:
    - type: file
      archive: false
      timeZone: system
      currentLogFilename: data/dd-transfer-to-vault.log
    - type: console
      # Used in combination with journald, which already adds the timestamp
      logFormat: "%-5p %c{0}: %m%n%dwREx"
  loggers:
    'org.hibernate.engine.internal.StatisticalLoggingSessionEventListener': 'OFF'
    'nl.knaw.dans.transfer.core.RemoveXmlFilesTask': 'OFF'
    'nl.knaw.dans.transfer.core.RemoveEmptyTargetDirsTask': 'OFF'
