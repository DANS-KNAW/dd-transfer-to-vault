{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"dd-transfer-to-vault \u00b6 Processes dataset version exports for inclusion in the DANS Data Vault Purpose \u00b6 This service is responsible for taking dataset version exports (DVE for short), cataloging them and transferring them to the DANS data vault. If the dataset version export is the first version of a dataset, an NBN persistent identifier is minted for the dataset and registered in the NBN database. Interfaces \u00b6 This service has the following interfaces: Provided interfaces \u00b6 Inbox \u00b6 Protocol type : Shared filesystem Internal or external : internal Purpose : to receive DVEs from the Data Stations and other services Command API \u00b6 Protocol type : HTTP Internal or external : internal Purpose : to interact with the service programmatically Admin console \u00b6 Protocol type : HTTP Internal or external : internal Purpose : application monitoring and management Consumed interfaces \u00b6 Validate Bagpack API \u00b6 Protocol type : HTTP Internal or external : internal Purpose : to validate that the DVE is a valid bagpack Extract metadata inbox \u00b6 Protocol type : Shared filesystem Internal or external : internal Purpose : to let the Validate Bagpack service access the DVE for validation Data Vault Catalog \u00b6 Protocol type : HTTP Internal or external : internal Purpose : to maintain information about the datasets and their versions that are stored in the DANS data vault NBN Database \u00b6 Protocol type : HTTP Internal or external : external Purpose : to mint and register NBN persistent identifiers for datasets Data Vault import inbox \u00b6 Protocol type : Shared filesystem Internal or external : internal Purpose : to import DVEs into the DANS data vault Data Vault API \u00b6 Protocol type : HTTP Internal or external : internal Purpose : to issue commands to the DANS data vault and retrieve information from it Processing \u00b6 This service is best viewed as a processing pipeline for DVEs. It connects a source that produces DVEs to a target DANS Data Vault Storage Root , which stores the DVEs as OCFL object versions. A source can be a Data Station or a Vault as a Service client. The service takes care of cataloging the DVEs and ensuring that the dataset is resolvable via the NBN persistent identifier. It attempts to do this in an efficient way, by processing multiple DVEs in parallel, while ensuring that the order of the dataset version exports is preserved. Furthermore, the service will attempt to resume processing of DVEs that were left unfinished in the event of a crash or restart. Inbox \u00b6 The inbox is a directory into which DVEs are dropped. When a DVE is detected the inbox will determine what the NBN of the target dataset is. DVEs for the same dataset version are processed in order, but DVEs for different dataset versions can be processed in parallel, except for the transfer to the vault (see below). Validation \u00b6 The first step in the processing pipeline is to validate the DVE. Currently, the only layout that is supported is the bagpack layout. If the DVE is not a bagpack, it will be rejected. Any other DVEs for the same dataset version will be blocked from processing until the problem is resolved. Metadata extraction \u00b6 The next step is to extract the metadata from the DVE and to create or update the dataset version in the DANS data vault catalog. The main source of metadata is the metadata/oai-ore.jsonld file in the DVE. NBN registration \u00b6 After the Vault Catalog has been updated, the NBN persistent identifier is minted and scheduled for registration in the NBN database. This is done in a separate background thread which uses a database table as a queue, so that the registration can be retried in case of a restart or crash. Transfer to vault \u00b6 Finally, the DVE is extracted to the current DANS data vault import inbox batch for this instance of dd-transfer-to-vault . If the size of the batch exceeds a configured threshold, the service will issue a command to the DANS data vault to import the current batch of DVEs. This step is executed on a single dedicated thread, so that determining the size of the batch can be done reliably.","title":"Description"},{"location":"#dd-transfer-to-vault","text":"Processes dataset version exports for inclusion in the DANS Data Vault","title":"dd-transfer-to-vault"},{"location":"#purpose","text":"This service is responsible for taking dataset version exports (DVE for short), cataloging them and transferring them to the DANS data vault. If the dataset version export is the first version of a dataset, an NBN persistent identifier is minted for the dataset and registered in the NBN database.","title":"Purpose"},{"location":"#interfaces","text":"This service has the following interfaces:","title":"Interfaces"},{"location":"#provided-interfaces","text":"","title":"Provided interfaces"},{"location":"#inbox","text":"Protocol type : Shared filesystem Internal or external : internal Purpose : to receive DVEs from the Data Stations and other services","title":"Inbox"},{"location":"#command-api","text":"Protocol type : HTTP Internal or external : internal Purpose : to interact with the service programmatically","title":"Command API"},{"location":"#admin-console","text":"Protocol type : HTTP Internal or external : internal Purpose : application monitoring and management","title":"Admin console"},{"location":"#consumed-interfaces","text":"","title":"Consumed interfaces"},{"location":"#validate-bagpack-api","text":"Protocol type : HTTP Internal or external : internal Purpose : to validate that the DVE is a valid bagpack","title":"Validate Bagpack API"},{"location":"#extract-metadata-inbox","text":"Protocol type : Shared filesystem Internal or external : internal Purpose : to let the Validate Bagpack service access the DVE for validation","title":"Extract metadata inbox"},{"location":"#data-vault-catalog","text":"Protocol type : HTTP Internal or external : internal Purpose : to maintain information about the datasets and their versions that are stored in the DANS data vault","title":"Data Vault Catalog"},{"location":"#nbn-database","text":"Protocol type : HTTP Internal or external : external Purpose : to mint and register NBN persistent identifiers for datasets","title":"NBN Database"},{"location":"#data-vault-import-inbox","text":"Protocol type : Shared filesystem Internal or external : internal Purpose : to import DVEs into the DANS data vault","title":"Data Vault import inbox"},{"location":"#data-vault-api","text":"Protocol type : HTTP Internal or external : internal Purpose : to issue commands to the DANS data vault and retrieve information from it","title":"Data Vault API"},{"location":"#processing","text":"This service is best viewed as a processing pipeline for DVEs. It connects a source that produces DVEs to a target DANS Data Vault Storage Root , which stores the DVEs as OCFL object versions. A source can be a Data Station or a Vault as a Service client. The service takes care of cataloging the DVEs and ensuring that the dataset is resolvable via the NBN persistent identifier. It attempts to do this in an efficient way, by processing multiple DVEs in parallel, while ensuring that the order of the dataset version exports is preserved. Furthermore, the service will attempt to resume processing of DVEs that were left unfinished in the event of a crash or restart.","title":"Processing"},{"location":"#inbox_1","text":"The inbox is a directory into which DVEs are dropped. When a DVE is detected the inbox will determine what the NBN of the target dataset is. DVEs for the same dataset version are processed in order, but DVEs for different dataset versions can be processed in parallel, except for the transfer to the vault (see below).","title":"Inbox"},{"location":"#validation","text":"The first step in the processing pipeline is to validate the DVE. Currently, the only layout that is supported is the bagpack layout. If the DVE is not a bagpack, it will be rejected. Any other DVEs for the same dataset version will be blocked from processing until the problem is resolved.","title":"Validation"},{"location":"#metadata-extraction","text":"The next step is to extract the metadata from the DVE and to create or update the dataset version in the DANS data vault catalog. The main source of metadata is the metadata/oai-ore.jsonld file in the DVE.","title":"Metadata extraction"},{"location":"#nbn-registration","text":"After the Vault Catalog has been updated, the NBN persistent identifier is minted and scheduled for registration in the NBN database. This is done in a separate background thread which uses a database table as a queue, so that the registration can be retried in case of a restart or crash.","title":"NBN registration"},{"location":"#transfer-to-vault","text":"Finally, the DVE is extracted to the current DANS data vault import inbox batch for this instance of dd-transfer-to-vault . If the size of the batch exceeds a configured threshold, the service will issue a command to the DANS data vault to import the current batch of DVEs. This step is executed on a single dedicated thread, so that determining the size of the batch can be done reliably.","title":"Transfer to vault"},{"location":"config/","text":"Configuration \u00b6 This module can be configured by editing the configuration file. This file is installed in /etc/opt/dans.knaw.nl/dd-transfer-to-vault/config.yml when using the RPM. The settings are explained with comments in the file itself. An on-line version of the latest configuration file can be found here .","title":"Configuration"},{"location":"config/#configuration","text":"This module can be configured by editing the configuration file. This file is installed in /etc/opt/dans.knaw.nl/dd-transfer-to-vault/config.yml when using the RPM. The settings are explained with comments in the file itself. An on-line version of the latest configuration file can be found here .","title":"Configuration"},{"location":"context/","text":"Context \u00b6 This module is a component in the DANS Data Station Architecture .","title":"Context"},{"location":"context/#context","text":"This module is a component in the DANS Data Station Architecture .","title":"Context"},{"location":"dev/","text":"Development \u00b6 General information about developing DANS modules can be found here . Local testing \u00b6 Local testing uses the same set-up as other DANS microservices. However, this service requires some other services to be running: dd-validate-bagpack dd-vault-catalog dd-data-vault What follows is a step-by-step instruction for how to process one test-deposit. This is not the only way you can do it, but it is a good starting point. Start dd-validate-bagpack . Make sure that validation.baseFolder is set to the root folder of the dd-transfer-to-vault module. If you are using the config.yml copied by start-env.sh , the validation.baseFolder assumes that the dd-transfer-to-vault module is located in the same parent directory. Start dd-vault-catalog . The default config.yml assumes that you have created a database named dd_vault_catalog_local_test on a machine called dev.transfer.dans-data.nl which is the case if you are using the DANS vagrant box for the transfer server. (Otherwise, you will have to create a database yourself and adjust the JDBC-config.) Start dd-data-vault . The default config.yml assumes that you have created a database named dd_data_vault_local_test on a machine called dev.transfer.dans-data.nl which is the case if you are using the DANS vagrant box for the transfer server. (Otherwise, you will have to create a database yourself and adjust the JDBC-config.) Start dd-transfer-to-vault itself. The working directories of the service are located under <dd-transfer-to-vault-root>/data/ : data \u251c\u2500\u2500 01_transfer-inbox \u2502 \u251c\u2500\u2500 failed \u2502 \u2514\u2500\u2500 inbox \u251c\u2500\u2500 02_extract-metadata \u2502 \u251c\u2500\u2500 inbox \u2502 \u2514\u2500\u2500 outbox \u2502 \u251c\u2500\u2500 failed \u2502 \u2514\u2500\u2500 rejected \u251c\u2500\u2500 03_send-to-vault \u2502 \u251c\u2500\u2500 inbox \u2502 \u251c\u2500\u2500 outbox \u2502 \u2502 \u251c\u2500\u2500 failed \u2502 \u2502 \u2514\u2500\u2500 processed \u2502 \u2514\u2500\u2500 work \u251c\u2500\u2500 04_data-vault \u2502 \u2514\u2500\u2500 inbox \u251c\u2500\u2500 dd-register-nbn \u2502 \u251c\u2500\u2500 failed \u2502 \u251c\u2500\u2500 inbox \u2502 \u2514\u2500\u2500 processed \u2514\u2500\u2500 dd-transfer-to-vault.log Once the service is running, you can copy test-DVEs to 01_transfer-inbox/inbox and follow in the logs (or with the debugger) what happens. In a happy-case scenario the DVE is collected from the inbox and placed in a directory named after the dataset's NBN in 02_extract-metadata/inbox . The extract-metadata task will then pick it up and process it, placing it in 03_send-to-vault/inbox , also in a directory named after the dataset's NBN. It will also place a request to register the NBN in dd-register-nbn/inbox , if the DVE is the first one for this dataset. Note that each step performs a ready check before starting to ensure that the services it depends on are available. For the send-to-vault step this means that the dd-data-vault service API must be available. If you don't want dd-data-vault to pick up the result of send-to-vault, just configure it to look in a different inbox directory (so not in 04_data-vault/inbox ). VaaS deposits require a skeleton record in the Vault Catalog \u00b6 In the Vault-as-a-Service pipeline a skeleton-record is created for the DVE as soon as it arrives. This then also assigns an OCFL object version number to the DVE by including it in the file name. The fact that the OCFL object version number is included in the name signals to dd-transfer-to-vault to update an existing record rather than creating a new one. Therefore, before put a VaaS-type DVE in the inbox, you must add a skeleton record for it in the Data Vault. This can be done with the dd-vault-catalog-cli :","title":"Overview"},{"location":"dev/#development","text":"General information about developing DANS modules can be found here .","title":"Development"},{"location":"dev/#local-testing","text":"Local testing uses the same set-up as other DANS microservices. However, this service requires some other services to be running: dd-validate-bagpack dd-vault-catalog dd-data-vault What follows is a step-by-step instruction for how to process one test-deposit. This is not the only way you can do it, but it is a good starting point. Start dd-validate-bagpack . Make sure that validation.baseFolder is set to the root folder of the dd-transfer-to-vault module. If you are using the config.yml copied by start-env.sh , the validation.baseFolder assumes that the dd-transfer-to-vault module is located in the same parent directory. Start dd-vault-catalog . The default config.yml assumes that you have created a database named dd_vault_catalog_local_test on a machine called dev.transfer.dans-data.nl which is the case if you are using the DANS vagrant box for the transfer server. (Otherwise, you will have to create a database yourself and adjust the JDBC-config.) Start dd-data-vault . The default config.yml assumes that you have created a database named dd_data_vault_local_test on a machine called dev.transfer.dans-data.nl which is the case if you are using the DANS vagrant box for the transfer server. (Otherwise, you will have to create a database yourself and adjust the JDBC-config.) Start dd-transfer-to-vault itself. The working directories of the service are located under <dd-transfer-to-vault-root>/data/ : data \u251c\u2500\u2500 01_transfer-inbox \u2502 \u251c\u2500\u2500 failed \u2502 \u2514\u2500\u2500 inbox \u251c\u2500\u2500 02_extract-metadata \u2502 \u251c\u2500\u2500 inbox \u2502 \u2514\u2500\u2500 outbox \u2502 \u251c\u2500\u2500 failed \u2502 \u2514\u2500\u2500 rejected \u251c\u2500\u2500 03_send-to-vault \u2502 \u251c\u2500\u2500 inbox \u2502 \u251c\u2500\u2500 outbox \u2502 \u2502 \u251c\u2500\u2500 failed \u2502 \u2502 \u2514\u2500\u2500 processed \u2502 \u2514\u2500\u2500 work \u251c\u2500\u2500 04_data-vault \u2502 \u2514\u2500\u2500 inbox \u251c\u2500\u2500 dd-register-nbn \u2502 \u251c\u2500\u2500 failed \u2502 \u251c\u2500\u2500 inbox \u2502 \u2514\u2500\u2500 processed \u2514\u2500\u2500 dd-transfer-to-vault.log Once the service is running, you can copy test-DVEs to 01_transfer-inbox/inbox and follow in the logs (or with the debugger) what happens. In a happy-case scenario the DVE is collected from the inbox and placed in a directory named after the dataset's NBN in 02_extract-metadata/inbox . The extract-metadata task will then pick it up and process it, placing it in 03_send-to-vault/inbox , also in a directory named after the dataset's NBN. It will also place a request to register the NBN in dd-register-nbn/inbox , if the DVE is the first one for this dataset. Note that each step performs a ready check before starting to ensure that the services it depends on are available. For the send-to-vault step this means that the dd-data-vault service API must be available. If you don't want dd-data-vault to pick up the result of send-to-vault, just configure it to look in a different inbox directory (so not in 04_data-vault/inbox ).","title":"Local testing"},{"location":"dev/#vaas-deposits-require-a-skeleton-record-in-the-vault-catalog","text":"In the Vault-as-a-Service pipeline a skeleton-record is created for the DVE as soon as it arrives. This then also assigns an OCFL object version number to the DVE by including it in the file name. The fact that the OCFL object version number is included in the name signals to dd-transfer-to-vault to update an existing record rather than creating a new one. Therefore, before put a VaaS-type DVE in the inbox, you must add a skeleton record for it in the Data Vault. This can be done with the dd-vault-catalog-cli :","title":"VaaS deposits require a skeleton record in the Vault Catalog"},{"location":"installation/","text":"Installation \u00b6 Currently, this project is built as an RPM package for RHEL8 and later. The RPM will install the binaries to /opt/dans.knaw.nl/dd-transfer-to-vault and the configuration files to /etc/opt/dans.knaw.nl/dd-transfer-to-vault . For installation on systems that do no support RPM and/or systemd: Build the tarball (see next section). Extract it to some location on your system, for example /opt/dans.knaw.nl/dd-transfer-to-vault . Start the service with the following command /opt/dans.knaw.nl/dd-transfer-to-vault/bin/dd-transfer-to-vault server /opt/dans.knaw.nl/dd-transfer-to-vault/cfg/config.yml Building from source \u00b6 Prerequisites: Java 17 or higher Maven 3.3.3 or higher RPM (optional, only if you want to build the RPM package) Steps: git clone https://github.com/DANS-KNAW/dd-transfer-to-vault.git cd dd-transfer-to-vault mvn clean install If the rpm executable is found at /usr/local/bin/rpm , the build profile that includes the RPM packaging will be activated. If rpm is available, but at a different path, then activate it by using Maven's -P switch: mvn -Pprm install . Alternatively, to build the tarball execute: mvn clean install assembly:single","title":"Installation"},{"location":"installation/#installation","text":"Currently, this project is built as an RPM package for RHEL8 and later. The RPM will install the binaries to /opt/dans.knaw.nl/dd-transfer-to-vault and the configuration files to /etc/opt/dans.knaw.nl/dd-transfer-to-vault . For installation on systems that do no support RPM and/or systemd: Build the tarball (see next section). Extract it to some location on your system, for example /opt/dans.knaw.nl/dd-transfer-to-vault . Start the service with the following command /opt/dans.knaw.nl/dd-transfer-to-vault/bin/dd-transfer-to-vault server /opt/dans.knaw.nl/dd-transfer-to-vault/cfg/config.yml","title":"Installation"},{"location":"installation/#building-from-source","text":"Prerequisites: Java 17 or higher Maven 3.3.3 or higher RPM (optional, only if you want to build the RPM package) Steps: git clone https://github.com/DANS-KNAW/dd-transfer-to-vault.git cd dd-transfer-to-vault mvn clean install If the rpm executable is found at /usr/local/bin/rpm , the build profile that includes the RPM packaging will be activated. If rpm is available, but at a different path, then activate it by using Maven's -P switch: mvn -Pprm install . Alternatively, to build the tarball execute: mvn clean install assembly:single","title":"Building from source"},{"location":"to-api/","text":"The API is defined in dd-transfer-to-vault-api . The version implemented by this service can be viewed in Swagger UI in a new tab: API .","title":"API"}]}